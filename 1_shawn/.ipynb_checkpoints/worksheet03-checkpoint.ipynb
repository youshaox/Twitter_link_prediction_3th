{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMP90051 Workshop 3\n",
    "## Logistic regression\n",
    "***\n",
    "In this workshop we'll be implementing L2-regularised logistic regression using `scipy` and `numpy`. \n",
    "Our key objectives are:\n",
    "\n",
    "* to become familiar with the optimisation problem that sits behind L2-regularised logistic regression;\n",
    "* to apply polynomial basis expansion and recognise when it's useful; and\n",
    "* to experiment with the effect of L2 regularisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Binary classification data\n",
    "Let's begin by generating some binary classification data.\n",
    "To make it easy for us to visualise the results, we'll stick to a two-dimensional feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, Y = make_circles(n_samples=300, noise=0.1, factor=0.7, random_state=90051)\n",
    "plt.plot(X[Y==0,0], X[Y==0,1], 'o', label = \"y=0\")\n",
    "plt.plot(X[Y==1,0], X[Y==1,1], 's', label = \"y=1\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"$x_0$\")\n",
    "plt.ylabel(\"$x_1$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** What's interesting about this data? Do you think logistic regression will perform well?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In preparation for fitting and evaluating a logistic regression model, we randomly partition the data into train/test sets. We use the `train_test_split` function from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.33, random_state=90051)\n",
    "print(\"Training set has {} instances. Test set has {} instances.\".format(X_train.shape[0], X_test.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Logistic regression objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lectures, that logistic regression models the distribution of the binary class $y$ *conditional* on the feature vector $\\mathbf{x}$ as\n",
    "$$\n",
    "y | \\mathbf{x} \\sim \\mathrm{Bernoulli}[\\sigma(\\mathbf{w}^T \\mathbf{x} + b)]\n",
    "$$\n",
    "where $\\mathbf{w}$ is the weight vector, $b$ is the bias term and $\\sigma(z) = 1/(1 + e^{-z})$ is the logistic function.\n",
    "To simplify the notation, we'll collect the model parameters $\\mathbf{w}$ and $b$ in a single vector $\\mathbf{v} = [b, \\mathbf{w}]$.\n",
    "\n",
    "Fitting this model amounts to choosing $\\mathbf{v}$ that minimises the sum of cross-entropies over the instances ($i = 1,\\ldots,n$) in the training set\n",
    "$$\n",
    "\\begin{align}\n",
    "    f_\\mathrm{cross-ent}(\\mathbf{v}; \\mathbf{X}, \\mathbf{Y}) = - \\sum_{i = 1}^{n} \\left\\{ y_i \\log \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) + (1 - y_i) \\log (1 - \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b)) \\right\\}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Often a regularisation term of the form $f_\\mathrm{reg}(\\mathbf{w}; \\lambda) = \\frac{1}{2} \\lambda \\mathbf{w}^T \\mathbf{w}$ is added to the objective to penalize large weights (this can help to prevent overfitting). Note that $\\lambda \\geq 0$ controls the strength of the regularisation term.\n",
    "\n",
    "Putting this together, our goal is to minimise the following objective function with respect to $\\mathbf{w}$ and $b$:\n",
    "$$\n",
    "f(\\mathbf{v}; \\mathbf{X}, \\mathbf{Y}, \\lambda) = f_\\mathrm{reg}(\\mathbf{w}; \\lambda) + f_\\mathrm{cross-ent}(\\mathbf{v}; \\mathbf{X}, \\mathbf{Y})\n",
    "$$\n",
    "\n",
    "**Question:** Why aren't we regularising the entire parameter vector $\\mathbf{v}$? Notice that only $\\mathbf{w}$ is included in $f_\\mathrm{reg}$â€”in other words $b$ is excluded from regularisation. \n",
    "\n",
    "We're going to find a solution to this minimisation problem using the BFGS algorithm (named after the inventors Broyden, Fletcher, Goldfarb and Shanno). BFGS is a \"hill-climbing\" algorithm like gradient descent, however it additionally makes use of second-order derivative information (by approximating the Hessian). It converges in fewer iterations than gradient descent (it's convergence rate is *superlinear* whereas gradient descent is only *linear*).\n",
    "\n",
    "We'll use an implementation of BFGS provided in `scipy` called `fmin_bfgs`. The algorithm requires two functions as input: (i) a function that evaluates the objective $f(\\mathbf{v}; \\ldots)$ and (ii) a function that evalutes the gradient $\\nabla_{\\mathbf{v}} f(\\mathbf{v}; \\ldots)$.\n",
    "\n",
    "Let's start by writing a function to compute $f(\\mathbf{v}; \\ldots)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit # this is the logistic function\n",
    "\n",
    "# v: parameter vector\n",
    "# X: feature matrix\n",
    "# Y: class labels\n",
    "# Lambda: regularisation constant\n",
    "def obj_fn(v, X, Y, Lambda):\n",
    "    prob_1 = expit(np.dot(X,v[1::]) + v[0])\n",
    "    reg_term = ... # fill in\n",
    "    cross_entropy_term = - np.dot(Y, np.log(prob_1)) - np.dot(1. - Y, np.log(1. - prob_1))\n",
    "    return ... # fill in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for the gradient, we use the following result (if you're familiar with vector calculus, you may wish to derive this yourself):\n",
    "$$\n",
    "\\nabla_{\\mathbf{v}} f(\\mathbf{v}; \\ldots) = \\left[\\frac{\\partial f(\\mathbf{w}, b;\\ldots)}{\\partial b}, \\nabla_{\\mathbf{w}} f(\\mathbf{w}, b; \\ldots) \\right] = \\left[\\sum_{i = 1}^{n} \\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) - y_i, \\sum_{i = 1}^{n} \\lambda \\mathbf{w} + (\\sigma(\\mathbf{w}^T \\mathbf{x}_i + b) - y_i)\\mathbf{x}_i\\right]\n",
    "$$\n",
    "\n",
    "The function below implements $\\nabla_{\\mathbf{v}} f(\\mathbf{v}; \\ldots)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v: parameter vector\n",
    "# X: feature matrix\n",
    "# Y: class labels\n",
    "# Lambda: regularisation constant\n",
    "def grad_obj_fn(v, X, Y, Lambda):\n",
    "    prob_1 = expit(np.dot(X, v[1::]) + v[0])\n",
    "    grad_b = np.sum(prob_1 - Y)\n",
    "    grad_w = Lambda * v[1::] + np.dot(prob_1 - Y, X)\n",
    "    return np.insert(grad_w, 0, grad_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Solving the minimization problem using BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've implemented functions to compute the objective and the gradient, we can plug them into `fmin_bfgs`.\n",
    "Specifically, we define a function `my_logistic_regression` which calls `fmin_bfgs` and returns the optimal weight vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.optimize import fmin_bfgs\n",
    "\n",
    "# X: feature matrix\n",
    "# Y: class labels\n",
    "# Lambda: regularisation constant\n",
    "# v_initial: initial guess for parameter vector\n",
    "def my_logistic_regression(X, Y, Lambda, v_initial, disp=True):\n",
    "    # Function for displaying progress\n",
    "    def display(v):\n",
    "        print('v is', v, 'objective is', obj_fn(v, X, Y, Lambda))\n",
    "    \n",
    "    return fmin_bfgs(f=obj_fn, fprime=grad_obj_fn, \n",
    "                     x0=v_initial, args=(X, Y, Lambda), disp=disp, \n",
    "                     callback=display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 1\n",
    "v_initial = ... # fill in a vector of zeros of appropriate length\n",
    "v_opt = my_logistic_regression(X_train, Y_train, Lambda, v_initial)\n",
    "\n",
    "# Function to plot the data points and decision boundary\n",
    "def plot_results(X, Y, v, trans_func = None):\n",
    "    # Scatter plot in feature space\n",
    "    plt.plot(X[Y==0,0], X[Y==0,1], 'o', label = \"y=0\")\n",
    "    plt.plot(X[Y==1,0], X[Y==1,1], 's', label = \"y=1\")\n",
    "    \n",
    "    # Compute axis limits\n",
    "    x0_lower = X[:,0].min() - 0.1\n",
    "    x0_upper = X[:,0].max() + 0.1\n",
    "    x1_lower = X[:,1].min() - 0.1\n",
    "    x1_upper = X[:,1].max() + 0.1\n",
    "    \n",
    "    # Generate grid over feature space\n",
    "    x0, x1 = np.mgrid[x0_lower:x0_upper:.01, x1_lower:x1_upper:.01]\n",
    "    grid = np.c_[x0.ravel(), x1.ravel()]\n",
    "    if (trans_func is not None):\n",
    "        grid = trans_func(grid) # apply transformation to features\n",
    "    arg = (np.dot(grid, v[1::]) + v[0]).reshape(x0.shape)\n",
    "    \n",
    "    # Plot decision boundary (where w^T x + b == 0)\n",
    "    plt.contour(x0, x1, arg, levels=[0], cmap=\"Greys\", vmin=-0.2, vmax=0.2)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "plot_results(X, Y, v_opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Is the solution what you expected? Is it a good fit for the data?\n",
    "\n",
    "**Question:** What's the accuracy of this model? Fill in the code below assuming the following decision function\n",
    "$$\n",
    "\\hat{y} = \\begin{cases}\n",
    "    1, &\\mathrm{if} \\ p(y = 1|\\mathbf{x}) \\geq \\tfrac{1}{2}, \\\\\n",
    "    0, &\\mathrm{otherwise}.\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Y_test_pred = ... # fill in\n",
    "accuracy_score(Y_test, Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Adding polynomial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've seen that ordinary logistic regression does poorly on this data set, because the data is not linearly separable in the $x_0,x_1$ feature space.\n",
    "\n",
    "We can get around this problem using basis expansion. In this case, we'll augment the feature space by adding polynomial features of degree 2. In other words, we replace the original feature matrix $\\mathbf{X}$ by a transformed feature matrix $\\mathbf{\\Phi}$ which contains additional columns corresponding to $x_0^2$, $x_0 x_1$ and $x_1^2$. This is done using the function `add_quadratic_features` defined below.\n",
    "\n",
    "**Note:** There's a built-in function in `sklearn` for adding polynomial features located at `sklearn.preprocessing.PolynomialFeatures`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: original feature matrix\n",
    "def add_quadratic_features(X):\n",
    "    return np.c_[X, X[:,0]**2, X[:,0]*X[:,1], X[:,1]**2]\n",
    "\n",
    "Phi_train = add_quadratic_features(X_train)\n",
    "Phi_test = add_quadratic_features(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's apply our custom logistic regression function again on the augmented feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Lambda = 1\n",
    "v_initial = ... # fill in a vector of zeros of appropriate length\n",
    "v_opt = my_logistic_regression(Phi_train, Y_train, Lambda, v_initial)\n",
    "plot_results(X, Y, v_opt, trans_func=add_quadratic_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we should get a better result for the accuracy on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Y_test_pred = ... # fill in\n",
    "accuracy_score(Y_test, Y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Effect of regularisation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we've fixed the regularisation constant so that $\\lambda = 1$. (Note it's possible to choose an \"optimal\" value for $\\lambda$ by applying cross-validation.)\n",
    "\n",
    "**Question:** What do you think will happen if we switch the regularisation off? Try setting $\\lambda$ to a small value (say $10^{-3}$) and check whether the accuracy of the model is affected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Logistic regression using sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have some insight into the optimisation problem behind logistic regression, you should feel confident in using the built-in implementation in `sklearn` (or other packages).\n",
    "Note that the `sklearn` implementation handles floating point underflow/overflow more carefully than we have done, and uses faster numerical optimisation algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(C=1)\n",
    "clf.fit(Phi_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "Y_test_pred = clf.predict(Phi_test)\n",
    "accuracy_score(Y_test, Y_test_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
