{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.sparse as sp\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import average_precision_score\n",
    "import pickle\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/\"\n",
    "train_data = data_dir + \"train.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def node2index():\n",
    "    node_index = dict()\n",
    "    with open(train_data) as f:\n",
    "        for line in f:\n",
    "            line = [x.strip('\\n') for x in line.split('\\t')]\n",
    "            node_index[line[0]] = node_index.get(line[0],[])+line[1:]\n",
    "            for node in line:\n",
    "                node_index[node] = node_index.get(node,[]) + [line[0]]\n",
    "    return node_index\n",
    "node_index = node2index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4867136"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculation\n",
    "def find_neighbours(id):\n",
    "    \n",
    "    \"\"\"\n",
    "    find all the neighbours of node by id.\n",
    "    1. All the sink node of node id will be appended as the neighbor first.\n",
    "    2. All the source node of node id will also be added as neighbor then.\n",
    "    3. return the neighbor set.\n",
    "    \"\"\"\n",
    "    neighbour_set = set()\n",
    "    for edge in sorted_edges:\n",
    "        if edge[0] == id:\n",
    "            # add the sink node of node id\n",
    "            neighbour_set.add(edge[1])\n",
    "        elif edge[1] == id:\n",
    "            # add the source node of node id\n",
    "            neighbour_set.add(edge[0])\n",
    "    return neighbour_set\n",
    "\n",
    "def get_jaccard_coefficient(node_x, node_y):\n",
    "    \"\"\"\n",
    "    in: node_x::Node object\n",
    "    in: node_y::Node object\n",
    "    return: jaccard's cofficient::numeric\n",
    "    \"\"\"\n",
    "    neigbours_set_of_node_x = set(node_index[str(node_x)])\n",
    "    neigbours_set_of_node_y = set(node_index[str(node_y)])\n",
    "    union_neighbours = neigbours_set_of_node_x | neigbours_set_of_node_y\n",
    "    common_neighbours = neigbours_set_of_node_x & neigbours_set_of_node_y\n",
    "    if len(union_neighbours)==0:\n",
    "        return 0.0\n",
    "    return(len(common_neighbours)/len(union_neighbours))\n",
    "\n",
    "def get_preferential_attachment(node_x, node_y):\n",
    "    neigbours_set_of_node_x = set(node_index[str(node_x)])\n",
    "    neigbours_set_of_node_y = set(node_index[str(node_y)])\n",
    "    return len(neigbours_set_of_node_x)*len(neigbours_set_of_node_y)\n",
    "\n",
    "def get_adamic_adar(node_x, node_y):\n",
    "    neigbours_set_of_node_x = set(node_index[str(node_x)])\n",
    "    neigbours_set_of_node_y = set(node_index[str(node_y)])\n",
    "    common_neighbours = neigbours_set_of_node_x & neigbours_set_of_node_y\n",
    "    # get the summation\n",
    "    score = 0\n",
    "    for common_node in common_neighbours:\n",
    "        score = score + 1/math.log(len(set(node_index[str(node_x)])))\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "with open(data_dir + \"test-public.txt\", \"r\") as f:\n",
    "     test_data = f.readlines()\n",
    "test_data = [i.split() for i in test_data[1:]]\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    make the prediction using the jaccard's coefficient\n",
    "    \"\"\"\n",
    "#     result = np.zeros()\n",
    "    id_list = []\n",
    "    new_result = []\n",
    "    count = 0\n",
    "    for line in test_data:\n",
    "        # converse to integer\n",
    "        node_x = int(line[1].strip())\n",
    "        node_y = int(line[2].strip())\n",
    "        jaccard_coefficient = get_jaccard_coefficient(node_x, node_y)\n",
    "        adamic_adar = get_adamic_adar(node_x, node_y)\n",
    "        preferential_attachment = get_preferential_attachment(node_x,node_y)\n",
    "        # 构成新的矩阵\n",
    "        new_result.append([jaccard_coefficient, adamic_adar, preferential_attachment])\n",
    "#         result = np.vstack([result, new_result])\n",
    "        id_list.append(line[0])\n",
    "    return id_list, new_result\n",
    "id_list, result = predict()\n",
    "# 预测结果\n",
    "result_array = np.asarray(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# data 需要为array\n",
    "def rescale_min_max(data): \n",
    "    \"\"\"\n",
    "    min-max normalisation\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    result = scaler.transform(data)\n",
    "    return result\n",
    "\n",
    "def standardise(data):\n",
    "    \"\"\"remove the mean and transform to unit variance\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    result = scaler.transform(data)\n",
    "    return result\n",
    "\n",
    "preprocessed_data = rescale_min_max(result_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 计算三个测度的均值，等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return the min, max, mean and median of different score of a single test edge.\n",
    "import statistics\n",
    "def get_min(input_row):\n",
    "    return min(input_row)\n",
    "\n",
    "def get_max(input_row):\n",
    "    return max(input_row)\n",
    "\n",
    "def get_mean(input_row):\n",
    "    return statistics.mean(input_row)\n",
    "\n",
    "def get_median(input_row):\n",
    "    return statistics.median(input_row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['1' '6.450000546912489e-07']\n",
      " ['2' '0.008333333333333333']\n",
      " ['3' '6.182489003492156e-07']\n",
      " ..., \n",
      " ['1998' '8.379650480117828e-05']\n",
      " ['1999' '1.8042167428460278e-05']\n",
      " ['2000' '2.95005785382979e-06']]\n"
     ]
    }
   ],
   "source": [
    "explantory_result = np.apply_along_axis(get_min, 1, preprocessed_data)\n",
    "id_array = np.transpose(np.asarray(id_list))\n",
    "# 拼接矩阵\n",
    "result_to_write = np.column_stack((id_array, explantory_result))\n",
    "print(result_to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the result to the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import time\n",
    "'''\n",
    "Description: get time\n",
    "Input: \n",
    "Output: time\n",
    "''' \n",
    "def nowtime():\n",
    "    return time.strftime(\"%Y%m%d-%H%M\", time.localtime())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Description: Save prediction result to files\n",
    "Input: (1) result\n",
    "       (2) filename\n",
    "Output: \n",
    "\"\"\"\n",
    "def save_prediction_to_csv(result,filename):\n",
    "    headers = ['id','Prediction']\n",
    "\n",
    "    with open(filename + str(nowtime()) + \".csv\", 'w', encoding = 'utf8') as f:\n",
    "        f_csv = csv.writer(f)\n",
    "        f_csv.writerow(headers)\n",
    "        f_csv.writerows(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_prediction_to_csv(result_to_write, \"shawn_normalise_min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
