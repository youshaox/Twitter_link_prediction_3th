{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/\"\n",
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(data_dir + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "training_data_v1 = load_obj(\"un_normalised_final_training_data_df_rf\")\n",
    "BasicFeatures = load_obj(\"pre_features-v2\")\n",
    "pre_features = BasicFeatures\n",
    "final_training_data_df = training_data_v1.iloc[:,3:30]\n",
    "final_labels_df = training_data_v1.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "#Salton Similarity\n",
    "def salton_similarity(node1, node2):\n",
    "    n1 = pre_features[node1]\n",
    "    n2 = pre_features[node2]\n",
    "    common_neighors = list(set(n1[2]).intersection(n2[2]))\n",
    "    inter = len(common_neighors)\n",
    "    degree_out_flow = n1[6]\n",
    "    degree_in_flow = n2[4]\n",
    "    \n",
    "    if inter == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        try:\n",
    "            sqrt_of_degree = math.sqrt(degree_out_flow * degree_in_flow)\n",
    "            salton = inter / sqrt_of_degree\n",
    "            probability = 1 /(1 - math.log(salton)*0.2)\n",
    "            return probability\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "#Cosine\n",
    "def Cosine(Node1, Node2):\n",
    "    n1 = pre_features[Node1]\n",
    "    n2 = pre_features[Node2]\n",
    "    common_neighors = list(set(n1[2]).intersection(n2[2]))\n",
    "    lm = len(common_neighors)\n",
    "    if lm == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (0.0+lm)/(len(n1[2])*len(n2[2]))\n",
    "\n",
    "def get_jaccard_coefficient(source, sink):\n",
    "    \"\"\"\n",
    "    in: source::Node object\n",
    "    in: sink::Node object\n",
    "    return: jaccard's cofficient::numeric\n",
    "    \"\"\"\n",
    "    # transform\n",
    "    neighbours_of_source_list = BasicFeatures[source][2]\n",
    "    neighbours_of_sink_list = BasicFeatures[sink][2]\n",
    "    \n",
    "    neigbours_set_of_source = set(neighbours_of_source_list)\n",
    "    neigbours_set_of_sink = set(neighbours_of_sink_list)\n",
    "    union_neighbours = neigbours_set_of_source | neigbours_set_of_sink\n",
    "    common_neighbours = neigbours_set_of_source & neigbours_set_of_sink\n",
    "    if len(union_neighbours)==0:\n",
    "        return 0.0\n",
    "    return(len(common_neighbours)/len(union_neighbours))\n",
    "\n",
    "def get_preferential_attachment(source, sink):\n",
    "    # transform\n",
    "    neighbours_of_source_list = BasicFeatures[source][2]\n",
    "    neighbours_of_sink_list = BasicFeatures[sink][2]\n",
    "    \n",
    "    neigbours_set_of_source = set(neighbours_of_source_list)\n",
    "    neigbours_set_of_sink = set(neighbours_of_sink_list)\n",
    "    \n",
    "    return len(neigbours_set_of_source)*len(neigbours_set_of_sink)\n",
    "\n",
    "def get_adamic_adar(source, sink):\n",
    "    # transform\n",
    "    neighbours_of_source_list = BasicFeatures[source][2]\n",
    "    neighbours_of_sink_list = BasicFeatures[sink][2]\n",
    "\n",
    "    neigbours_set_of_source = set(neighbours_of_source_list)\n",
    "    neigbours_set_of_sink = set(neighbours_of_sink_list)\n",
    "    common_neighbours = neigbours_set_of_source & neigbours_set_of_sink\n",
    "    # get the summation\n",
    "    score = 0\n",
    "    for common_node in common_neighbours:\n",
    "        if math.log(len(BasicFeatures[common_node][2])) == 0:\n",
    "            return 0.0\n",
    "        score = score + 1/math.log(len(BasicFeatures[common_node][2]))\n",
    "    return score\n",
    "\n",
    "def get_resource_allocation(source, sink):\n",
    "    neighbours_of_source_list = BasicFeatures[source][2]\n",
    "    neighbours_of_sink_list = BasicFeatures[sink][2]\n",
    "#     print(neighbours_of_source_list)\n",
    "#     print(neighbours_of_sink_list)\n",
    "    neigbours_set_of_source = set(neighbours_of_source_list)\n",
    "    neigbours_set_of_sink = set(neighbours_of_sink_list)\n",
    "    \n",
    "    common_neighbours = neigbours_set_of_source & neigbours_set_of_sink\n",
    "#     print(common_neighbours)\n",
    "    score=0\n",
    "    for common_node in common_neighbours:\n",
    "        # number of the neighbours of the common_node\n",
    "        try:\n",
    "            single_common_node_score = 1/BasicFeatures[common_node][0]\n",
    "        except:\n",
    "            single_common_node_score=0\n",
    "        score = score + single_common_node_score\n",
    "    return score\n",
    "    \n",
    "\n",
    "# how similar are the outbound neighbors of source to sink\n",
    "# either JA, PA, AA\n",
    "def get_outbound_similarity_score(source, sink, metric):\n",
    "    # get the outbound_node of source\n",
    "    outbound_node_for_source_set = set(BasicFeatures[source][5])\n",
    "    summation = 0\n",
    "    for outbound_node_for_source in outbound_node_for_source_set:\n",
    "        summation =summation + metric(sink,outbound_node_for_source)\n",
    "    if len(outbound_node_for_source_set) == 0:\n",
    "        return 0\n",
    "    score = 1/len(outbound_node_for_source_set)*summation\n",
    "    return score\n",
    "\n",
    "# either JA, PA, AA\n",
    "def get_inbound_similarity_score(source, sink, metric):\n",
    "    # get the inbound_node of sink\n",
    "    inbound_node_for_sink_set = set(BasicFeatures[sink][3])\n",
    "    summation = 0\n",
    "    for inbound_node_for_sink in inbound_node_for_sink_set:\n",
    "        summation =summation + metric(source,inbound_node_for_sink)\n",
    "    if len(inbound_node_for_sink_set) == 0:\n",
    "        return 0\n",
    "    score = 1/len(inbound_node_for_sink_set)*summation\n",
    "    return score\n",
    "\n",
    "def get_common_neighbours(node1, node2):\n",
    "    try:\n",
    "        n1 = pre_features[node1]\n",
    "        n2 = pre_features[node2]\n",
    "        common_neighors = list(set(n1[2]).intersection(n2[2]))\n",
    "        return common_neighors\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "# data 需要为array\n",
    "def rescale_min_max(data): \n",
    "    \"\"\"\n",
    "    min-max normalisation\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    result = scaler.transform(data)\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def standardise(data):\n",
    "    \"\"\"remove the mean and transform to unit variance\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    result = scaler.transform(data)\n",
    "    return pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the training data\n",
    "# training_df = get_training_df(labeled_edges)\n",
    "\n",
    "final_training_data_df_former = training_data_v1.iloc[:,3:30]\n",
    "final_labels_df = training_data_v1.iloc[:,2]\n",
    "training_df= training_data_v1\n",
    "\n",
    "final_labels_df = training_df.iloc[:,2]\n",
    "final_training_data_df = final_training_data_df_former.iloc[:,9:15]\n",
    "# 使用标准化\n",
    "# final_training_data_df = rescale_min_max(measurement_to_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    }
   ],
   "source": [
    "X=final_training_data_df\n",
    "# count=0\n",
    "# get the data and label\n",
    "y=final_labels_df\n",
    "\n",
    "# training model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_t, X_test, y_t, y_test = train_test_split(X,y)\n",
    "X_train, X_validation, y_train, y_validation  = train_test_split(X_t,y_t)\n",
    "# Gridsearch settings\n",
    "pipeline = Pipeline([\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "# 0.01, 0.1, 1, 5, 10\n",
    "parameters = {\n",
    "       'clf__penalty': ('l1','l2'),\n",
    "       'clf__C': (0.1, 0.01, 10)\n",
    " }\n",
    "# 1. training_df_10w running\n",
    "X_train = X_t\n",
    "y_train = y_t\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,\n",
    "   verbose=1, scoring='roc_auc', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: %0.3f' % grid_search.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "predictions = grid_search.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))\n",
    "print('Precision:', precision_score(y_test, predictions))\n",
    "print('Recall:', recall_score(y_test, predictions))\n",
    "\n",
    "\n",
    "\n",
    "# make the prediction\n",
    "from tqdm import tqdm\n",
    "with open(data_dir + \"test-public.txt\", \"r\") as f:\n",
    "     test_data = f.readlines()\n",
    "test_data = [i.split() for i in test_data[1:]]\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    make the prediction using the jaccard's coefficient\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for line in tqdm(test_data, mininterval=5):\n",
    "        # converse to integer\n",
    "        source = int(line[1].strip())\n",
    "        sink = int(line[2].strip())\n",
    "#         common_neighbours = get_common_neighbours(source,sink)\n",
    "#         num_of_neighbours_source=BasicFeatures[source][0]\n",
    "#         num_of_in_neighbours_source=BasicFeatures[source][4]\n",
    "#         num_of_out_neighbours_source=BasicFeatures[source][6]\n",
    "\n",
    "#         num_of_neighbours_sink=BasicFeatures[sink][0]\n",
    "#         num_of_in_neighbours_sink=BasicFeatures[sink][4]\n",
    "#         num_of_out_neighbours_sink=BasicFeatures[sink][6]\n",
    "        \n",
    "#         num_of_neighbours_sum=BasicFeatures[source][0] + BasicFeatures[sink][0]\n",
    "#         num_of_in_neighbours_sum=BasicFeatures[source][4] + BasicFeatures[sink][4]\n",
    "#         num_of_out_neighbours_sum=BasicFeatures[source][6] + BasicFeatures[sink][6]\n",
    "        salton_similarity_score = salton_similarity(source, sink)\n",
    "        cosine = Cosine(source, sink)\n",
    "        jaccard_coefficient = get_jaccard_coefficient(source, sink)\n",
    "        preferential_attachment = get_preferential_attachment(source, sink)\n",
    "        adamic_adar = get_adamic_adar(source, sink)\n",
    "        resource_allocation = get_resource_allocation(source, sink)\n",
    "\n",
    "        X_test = pd.DataFrame([\n",
    "#                                num_of_neighbours_source,\n",
    "#                                num_of_in_neighbours_source,\n",
    "#                                num_of_out_neighbours_source,\n",
    "#                                num_of_neighbours_sink,\n",
    "#                                num_of_in_neighbours_sink,\n",
    "#                                num_of_out_neighbours_sink,\n",
    "#                                num_of_neighbours_sum,\n",
    "#                                num_of_in_neighbours_sum,\n",
    "#                                num_of_out_neighbours_sum,      \n",
    "                               salton_similarity_score, \n",
    "                               cosine, \n",
    "                               jaccard_coefficient,\n",
    "                               preferential_attachment, \n",
    "                               adamic_adar, \n",
    "                               resource_allocation\n",
    "                              ]).T\n",
    "#         grid_search.predict_proba()\n",
    "        single_result = grid_search.predict_proba(X_test)[0]\n",
    "        result.append((line[0], single_result))\n",
    "    return result\n",
    "result = predict()\n",
    "\n",
    "\n",
    "# save the result\n",
    "\n",
    "import csv\n",
    "import time\n",
    "'''\n",
    "Description: get time\n",
    "Input: \n",
    "Output: time\n",
    "''' \n",
    "def nowtime():\n",
    "    return time.strftime(\"%Y%m%d-%H%M\", time.localtime())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Description: Save prediction result to files\n",
    "Input: (1) result\n",
    "       (2) filename\n",
    "Output: \n",
    "\"\"\"\n",
    "def save_prediction_to_csv(result,filename):\n",
    "    headers = ['id','Prediction']\n",
    "\n",
    "    with open(filename + str(nowtime()) + \".csv\", 'w', encoding = 'utf8') as f:\n",
    "        f_csv = csv.writer(f)\n",
    "        f_csv.writerow(headers)\n",
    "        f_csv.writerows(result)\n",
    "save_prediction_to_csv(result, \"shawn_lr_6_no_test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the result of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'best_score_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-c1b8490e3b2a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best score: %0.3f'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best parameters set:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbest_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mparam_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'best_score_'"
     ]
    }
   ],
   "source": [
    "print('Best score: %0.3f' % grid_search.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('\\t%s: %r' % (param_name, best_parameters[param_name]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 考虑画图\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please save the training set as the csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Cs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-286263b4acc5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrid_scores_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGammas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Cs' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "scores = [x[1] for x in grid_search.grid_scores_]\n",
    "scores = np.array(scores).reshape(len(Cs), len(Gammas))\n",
    "\n",
    "for ind, i in enumerate(Cs):\n",
    "    plt.plot(Gammas, scores[ind], label='C: ' + str(i))\n",
    "plt.legend()\n",
    "plt.xlabel('Gamma')\n",
    "plt.ylabel('Mean score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'cv_results_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-22fb0c517cac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Calling Method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mplot_grid_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcv_results_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_estimators\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'N Estimators'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Max Features'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'cv_results_'"
     ]
    }
   ],
   "source": [
    "def plot_grid_search(cv_results, grid_param_1, grid_param_2, name_param_1, name_param_2):\n",
    "    # Get Test Scores Mean and std for each grid search\n",
    "    scores_mean = cv_results['mean_test_score']\n",
    "    scores_mean = np.array(scores_mean).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    scores_sd = cv_results['std_test_score']\n",
    "    scores_sd = np.array(scores_sd).reshape(len(grid_param_2),len(grid_param_1))\n",
    "\n",
    "    # Plot Grid search scores\n",
    "    _, ax = plt.subplots(1,1)\n",
    "\n",
    "    # Param1 is the X-axis, Param 2 is represented as a different curve (color line)\n",
    "    for idx, val in enumerate(grid_param_2):\n",
    "        ax.plot(grid_param_1, scores_mean[idx,:], '-o', label= name_param_2 + ': ' + str(val))\n",
    "\n",
    "    ax.set_title(\"Grid Search Scores\", fontsize=20, fontweight='bold')\n",
    "    ax.set_xlabel(name_param_1, fontsize=16)\n",
    "    ax.set_ylabel('CV Average Score', fontsize=16)\n",
    "    ax.legend(loc=\"best\", fontsize=15)\n",
    "    ax.grid('on')\n",
    "\n",
    "# Calling Method \n",
    "plot_grid_search(grid_search.cv_results_, n_estimators, max_features, 'N Estimators', 'Max Features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sklearn.grid_search.GridSearchCV"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_cache',\n",
       " '_abc_negative_cache',\n",
       " '_abc_negative_cache_version',\n",
       " '_abc_registry',\n",
       " '_estimator_type',\n",
       " '_fit',\n",
       " '_get_param_names',\n",
       " 'classes_',\n",
       " 'cv',\n",
       " 'decision_function',\n",
       " 'error_score',\n",
       " 'estimator',\n",
       " 'fit',\n",
       " 'fit_params',\n",
       " 'get_params',\n",
       " 'iid',\n",
       " 'inverse_transform',\n",
       " 'n_jobs',\n",
       " 'param_grid',\n",
       " 'pre_dispatch',\n",
       " 'predict',\n",
       " 'predict_log_proba',\n",
       " 'predict_proba',\n",
       " 'refit',\n",
       " 'score',\n",
       " 'scorer_',\n",
       " 'scoring',\n",
       " 'set_params',\n",
       " 'transform',\n",
       " 'verbose']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(grid_search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'GridSearchCV' object has no attribute 'export_graphviz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-4a83e2f43033>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"iris_tree.dot\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport_graphviz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdt_rf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'GridSearchCV' object has no attribute 'export_graphviz'"
     ]
    }
   ],
   "source": [
    "f = open(\"iris_tree.dot\", 'w')     \n",
    "grid_search.export_graphviz(dt_rf, out_file=f) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sklearn.grid_search.BaseSearchCV.predict(self, X)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
