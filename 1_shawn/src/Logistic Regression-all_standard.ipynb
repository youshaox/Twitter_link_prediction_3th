{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model.logistic import LogisticRegression\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../../data/\"\n",
    "def save_obj(obj, name ):\n",
    "    with open( name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name):\n",
    "    with open(data_dir + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "training_data_v1 = load_obj(\"un_normalised_final_training_data_df_rf\")\n",
    "BasicFeatures = load_obj(\"pre_features-v2\")\n",
    "pre_features = BasicFeatures\n",
    "final_training_data_df = training_data_v1.iloc[:,3:30]\n",
    "final_labels_df = training_data_v1.iloc[:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function\n",
    "#Salton Similarity\n",
    "def salton_similarity(node1, node2):\n",
    "    n1 = pre_features[node1]\n",
    "    n2 = pre_features[node2]\n",
    "    common_neighors = list(set(n1[2]).intersection(n2[2]))\n",
    "    inter = len(common_neighors)\n",
    "    degree_out_flow = n1[6]\n",
    "    degree_in_flow = n2[4]\n",
    "    \n",
    "    if inter == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        try:\n",
    "            sqrt_of_degree = math.sqrt(degree_out_flow * degree_in_flow)\n",
    "            salton = inter / sqrt_of_degree\n",
    "            probability = 1 /(1 - math.log(salton)*0.2)\n",
    "            return probability\n",
    "        except:\n",
    "            return 0\n",
    "\n",
    "#Cosine\n",
    "def Cosine(Node1, Node2):\n",
    "    n1 = pre_features[Node1]\n",
    "    n2 = pre_features[Node2]\n",
    "    common_neighors = list(set(n1[2]).intersection(n2[2]))\n",
    "    lm = len(common_neighors)\n",
    "    if lm == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return (0.0+lm)/(len(n1[2])*len(n2[2]))\n",
    "\n",
    "def get_jaccard_coefficient(source, sink):\n",
    "    \"\"\"\n",
    "    in: source::Node object\n",
    "    in: sink::Node object\n",
    "    return: jaccard's cofficient::numeric\n",
    "    \"\"\"\n",
    "    # transform\n",
    "    neighbours_of_source_list = BasicFeatures[source][2]\n",
    "    neighbours_of_sink_list = BasicFeatures[sink][2]\n",
    "    \n",
    "    neigbours_set_of_source = set(neighbours_of_source_list)\n",
    "    neigbours_set_of_sink = set(neighbours_of_sink_list)\n",
    "    union_neighbours = neigbours_set_of_source | neigbours_set_of_sink\n",
    "    common_neighbours = neigbours_set_of_source & neigbours_set_of_sink\n",
    "    if len(union_neighbours)==0:\n",
    "        return 0.0\n",
    "    return(len(common_neighbours)/len(union_neighbours))\n",
    "\n",
    "def get_preferential_attachment(source, sink):\n",
    "    # transform\n",
    "    neighbours_of_source_list = BasicFeatures[source][2]\n",
    "    neighbours_of_sink_list = BasicFeatures[sink][2]\n",
    "    \n",
    "    neigbours_set_of_source = set(neighbours_of_source_list)\n",
    "    neigbours_set_of_sink = set(neighbours_of_sink_list)\n",
    "    \n",
    "    return len(neigbours_set_of_source)*len(neigbours_set_of_sink)\n",
    "\n",
    "def get_adamic_adar(source, sink):\n",
    "    # transform\n",
    "    neighbours_of_source_list = BasicFeatures[source][2]\n",
    "    neighbours_of_sink_list = BasicFeatures[sink][2]\n",
    "\n",
    "    neigbours_set_of_source = set(neighbours_of_source_list)\n",
    "    neigbours_set_of_sink = set(neighbours_of_sink_list)\n",
    "    common_neighbours = neigbours_set_of_source & neigbours_set_of_sink\n",
    "    # get the summation\n",
    "    score = 0\n",
    "    for common_node in common_neighbours:\n",
    "        if math.log(len(BasicFeatures[common_node][2])) == 0:\n",
    "            return 0.0\n",
    "        score = score + 1/math.log(len(BasicFeatures[common_node][2]))\n",
    "    return score\n",
    "\n",
    "def get_resource_allocation(source, sink):\n",
    "    neighbours_of_source_list = BasicFeatures[source][2]\n",
    "    neighbours_of_sink_list = BasicFeatures[sink][2]\n",
    "#     print(neighbours_of_source_list)\n",
    "#     print(neighbours_of_sink_list)\n",
    "    neigbours_set_of_source = set(neighbours_of_source_list)\n",
    "    neigbours_set_of_sink = set(neighbours_of_sink_list)\n",
    "    \n",
    "    common_neighbours = neigbours_set_of_source & neigbours_set_of_sink\n",
    "#     print(common_neighbours)\n",
    "    score=0\n",
    "    for common_node in common_neighbours:\n",
    "        # number of the neighbours of the common_node\n",
    "        try:\n",
    "            single_common_node_score = 1/BasicFeatures[common_node][0]\n",
    "        except:\n",
    "            single_common_node_score=0\n",
    "        score = score + single_common_node_score\n",
    "    return score\n",
    "    \n",
    "\n",
    "# how similar are the outbound neighbors of source to sink\n",
    "# either JA, PA, AA\n",
    "def get_outbound_similarity_score(source, sink, metric):\n",
    "    # get the outbound_node of source\n",
    "    outbound_node_for_source_set = set(BasicFeatures[source][5])\n",
    "    summation = 0\n",
    "    for outbound_node_for_source in outbound_node_for_source_set:\n",
    "        summation =summation + metric(sink,outbound_node_for_source)\n",
    "    if len(outbound_node_for_source_set) == 0:\n",
    "        return 0\n",
    "    score = 1/len(outbound_node_for_source_set)*summation\n",
    "    return score\n",
    "\n",
    "# either JA, PA, AA\n",
    "def get_inbound_similarity_score(source, sink, metric):\n",
    "    # get the inbound_node of sink\n",
    "    inbound_node_for_sink_set = set(BasicFeatures[sink][3])\n",
    "    summation = 0\n",
    "    for inbound_node_for_sink in inbound_node_for_sink_set:\n",
    "        summation =summation + metric(source,inbound_node_for_sink)\n",
    "    if len(inbound_node_for_sink_set) == 0:\n",
    "        return 0\n",
    "    score = 1/len(inbound_node_for_sink_set)*summation\n",
    "    return score\n",
    "\n",
    "def get_common_neighbours(node1, node2):\n",
    "    try:\n",
    "        n1 = pre_features[node1]\n",
    "        n2 = pre_features[node2]\n",
    "        common_neighors = list(set(n1[2]).intersection(n2[2]))\n",
    "        return common_neighors\n",
    "    except:\n",
    "        return 0\n",
    "\n",
    "def get_training_df(final_edges):\n",
    "    training_df = pd.DataFrame()\n",
    "    for edge in tqdm(final_edges):\n",
    "        source = edge[0]\n",
    "        sink = edge[1]\n",
    "        label = edge[2]\n",
    "        common_neighbours = get_common_neighbours(source,sink)\n",
    "        num_of_neighbours_source=BasicFeatures[source][0]\n",
    "        num_of_in_neighbours_source=BasicFeatures[source][4]\n",
    "        num_of_out_neighbours_source=BasicFeatures[source][6]\n",
    "\n",
    "        num_of_neighbours_sink=BasicFeatures[sink][0]\n",
    "        num_of_in_neighbours_sink=BasicFeatures[sink][4]\n",
    "        num_of_out_neighbours_sink=BasicFeatures[sink][6]\n",
    "        \n",
    "        num_of_neighbours_sum=BasicFeatures[source][0] + BasicFeatures[sink][0]\n",
    "        num_of_in_neighbours_sum=BasicFeatures[source][4] + BasicFeatures[sink][4]\n",
    "        num_of_out_neighbours_sum=BasicFeatures[source][6] + BasicFeatures[sink][6]\n",
    "        \n",
    "        salton_similarity_score = salton_similarity(source, sink)\n",
    "        cosine = Cosine(source, sink)\n",
    "        jaccard_coefficient = get_jaccard_coefficient(source, sink)\n",
    "        preferential_attachment = get_preferential_attachment(source, sink)\n",
    "        adamic_adar = get_adamic_adar(source, sink)\n",
    "        resource_allocation = get_resource_allocation(source, sink)\n",
    "\n",
    "#         salton_similarity_score_out = get_outbound_similarity_score(source, sink, salton_similarity)\n",
    "#         cosine_out = get_outbound_similarity_score(source, sink, Cosine)\n",
    "#         jaccard_coefficient_out = get_outbound_similarity_score(source, sink, get_jaccard_coefficient)\n",
    "#         preferential_attachment_out = get_outbound_similarity_score(source, sink, get_preferential_attachment)\n",
    "#         adamic_adar_out = get_outbound_similarity_score(source, sink, get_adamic_adar)\n",
    "#         resource_allocation_out = get_outbound_similarity_score(source, sink, get_resource_allocation)\n",
    "\n",
    "#         salton_similarity_score_in = get_inbound_similarity_score(source, sink, salton_similarity)\n",
    "#         cosine_in = get_inbound_similarity_score(source, sink, Cosine)\n",
    "#         jaccard_coefficient_in = get_inbound_similarity_score(source, sink, get_jaccard_coefficient)\n",
    "#         preferential_attachment_in = get_inbound_similarity_score(source, sink, get_preferential_attachment)\n",
    "#         adamic_adar_in = get_inbound_similarity_score(source, sink, get_adamic_adar)\n",
    "#         resource_allocation_in = get_inbound_similarity_score(source, sink, get_resource_allocation)\n",
    "\n",
    "# add the basic features\n",
    "        df_row = pd.DataFrame([\n",
    "                               source, \n",
    "                               sink, \n",
    "                               label,\n",
    "                               num_of_neighbours_source,\n",
    "                               num_of_in_neighbours_source,\n",
    "                               num_of_out_neighbours_source,\n",
    "                               num_of_neighbours_sink,\n",
    "                               num_of_in_neighbours_sink,\n",
    "                               num_of_out_neighbours_sink,\n",
    "                               num_of_neighbours_sum,\n",
    "                               num_of_in_neighbours_sum,\n",
    "                               num_of_out_neighbours_sum,      \n",
    "                               salton_similarity_score, \n",
    "                               cosine, \n",
    "                               jaccard_coefficient,\n",
    "                               preferential_attachment, \n",
    "                               adamic_adar, \n",
    "                               resource_allocation\n",
    "#                                salton_similarity_score_out,\n",
    "#                                cosine_out,\n",
    "#                                jaccard_coefficient_out,\n",
    "#                                preferential_attachment_out,\n",
    "#                                adamic_adar_out,\n",
    "#                                resource_allocation_out\n",
    "#                                salton_similarity_score_in,\n",
    "#                                cosine_in,\n",
    "#                                jaccard_coefficient_in,\n",
    "#                                preferential_attachment_in,\n",
    "#                                adamic_adar_in,\n",
    "#                                resource_allocation_in\n",
    "                              ]).T\n",
    "        training_df = training_df.append(df_row)\n",
    "    training_df.rename(columns={\n",
    "        0:'source', \n",
    "        1:'sink', \n",
    "        2:'label',\n",
    "        3:'num_of_neighbours_source',\n",
    "        4:'num_of_in_neighbours_source',\n",
    "        5:'num_of_out_neighbours_source',\n",
    "        6:'num_of_neighbours_sink',\n",
    "        7:'num_of_in_neighbours_sink',\n",
    "        8:'num_of_out_neighbours_sink',\n",
    "        9:'num_of_neighbours_sum',\n",
    "        10:'num_of_in_neighbours_sum',\n",
    "        11:'num_of_out_neighbours_sum',      \n",
    "        12:'salton_similarity_score', \n",
    "        13:'cosine', \n",
    "        14:'jaccard_coefficient',\n",
    "        15:'preferential_attachment', \n",
    "        16:'adamic_adar', \n",
    "        17:'resource_allocation'\n",
    "#         18:'salton_similarity_score_out',\n",
    "#         19:'cosine_out',\n",
    "#         20:'jaccard_coefficient_out',\n",
    "#         21:'preferential_attachment_out',\n",
    "#         22:'adamic_adar_out',\n",
    "#         23:'resource_allocation_out'\n",
    "#         25:'salton_similarity_score_in',\n",
    "#         26:'cosine_in',\n",
    "#         27:'jaccard_coefficient_in',\n",
    "#         28:'preferential_attachment_in',\n",
    "#         29:'adamic_adar_in',\n",
    "#         30:'resource_allocation_in'           \n",
    "    },inplace=True)\n",
    "    training_df[['source', 'sink', 'label']] = training_df[['source', 'sink', 'label']].astype(int)\n",
    "    return training_df\n",
    "\n",
    "# data 需要为array\n",
    "def rescale_min_max(data): \n",
    "    \"\"\"\n",
    "    min-max normalisation\n",
    "    \"\"\"\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler.fit(data)\n",
    "    result = scaler.transform(data)\n",
    "    return pd.DataFrame(result)\n",
    "\n",
    "def standardise(data):\n",
    "    \"\"\"remove the mean and transform to unit variance\"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(data)\n",
    "    result = scaler.transform(data)\n",
    "    return pd.DataFrame(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise the training data\n",
    "\n",
    "final_training_data_df = training_data_v1.iloc[:,3:30]\n",
    "final_labels_df = training_data_v1.iloc[:,2]\n",
    "training_df= training_data_v1\n",
    "final_labels_df = training_df.iloc[:,2]\n",
    "measurement_to_normal = training_df.iloc[:,3:30]\n",
    "# 使用标准化\n",
    "final_training_data_df = standardise(measurement_to_normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-4:\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 916, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/usr/lib/python3.6/threading.py\", line 864, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 405, in _handle_workers\n",
      "    pool._maintain_pool()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 246, in _maintain_pool\n",
      "    self._repopulate_pool()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/pool.py\", line 239, in _repopulate_pool\n",
      "    w.start()\n",
      "  File \"/usr/lib/python3.6/multiprocessing/process.py\", line 105, in start\n",
      "    self._popen = self._Popen(self)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/context.py\", line 277, in _Popen\n",
      "    return Popen(process_obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 19, in __init__\n",
      "    self._launch(process_obj)\n",
      "  File \"/usr/lib/python3.6/multiprocessing/popen_fork.py\", line 66, in _launch\n",
      "    self.pid = os.fork()\n",
      "OSError: [Errno 12] Cannot allocate memory\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X=final_training_data_df\n",
    "# count=0\n",
    "# get the data and label\n",
    "y=final_labels_df\n",
    "\n",
    "# training model\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_t, X_test, y_t, y_test = train_test_split(X,y)\n",
    "X_train, X_validation, y_train, y_validation  = train_test_split(X_t,y_t)\n",
    "# Gridsearch settings\n",
    "pipeline = Pipeline([\n",
    "    ('clf', LogisticRegression())\n",
    "])\n",
    "# 0.01, 0.1, 1, 5, 10\n",
    "parameters = {\n",
    "       'clf__penalty': ('l1','l2'),\n",
    "       'clf__C': (0.1, 0.01, 10)\n",
    " }\n",
    "# 1. training_df_10w running\n",
    "X_train = X_t\n",
    "y_train = y_t\n",
    "grid_search = GridSearchCV(pipeline, parameters, n_jobs=-1,\n",
    "   verbose=1, scoring='roc_auc', cv=3)\n",
    "grid_search.fit(X_train, y_train)\n",
    "print('Best score: %0.3f' % grid_search.best_score_)\n",
    "print('Best parameters set:')\n",
    "best_parameters = grid_search.best_estimator_.get_params()\n",
    "\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print('\\t%s: %r' % (param_name, best_parameters[param_name]))\n",
    "predictions = grid_search.predict(X_test)\n",
    "print('Accuracy:', accuracy_score(y_test, predictions))\n",
    "print('Precision:', precision_score(y_test, predictions))\n",
    "print('Recall:', recall_score(y_test, predictions))\n",
    "\n",
    "\n",
    "\n",
    "# make the prediction\n",
    "from tqdm import tqdm\n",
    "with open(data_dir + \"test-public.txt\", \"r\") as f:\n",
    "     test_data = f.readlines()\n",
    "test_data = [i.split() for i in test_data[1:]]\n",
    "\n",
    "def predict():\n",
    "    \"\"\"\n",
    "    make the prediction using the jaccard's coefficient\n",
    "    \"\"\"\n",
    "    result = []\n",
    "    for line in tqdm(test_data, mininterval=5):\n",
    "        # converse to integer\n",
    "        source = int(line[1].strip())\n",
    "        sink = int(line[2].strip())\n",
    "        common_neighbours = get_common_neighbours(source,sink)\n",
    "        num_of_neighbours_source=BasicFeatures[source][0]\n",
    "        num_of_in_neighbours_source=BasicFeatures[source][4]\n",
    "        num_of_out_neighbours_source=BasicFeatures[source][6]\n",
    "\n",
    "        num_of_neighbours_sink=BasicFeatures[sink][0]\n",
    "        num_of_in_neighbours_sink=BasicFeatures[sink][4]\n",
    "        num_of_out_neighbours_sink=BasicFeatures[sink][6]\n",
    "        \n",
    "        num_of_neighbours_sum=BasicFeatures[source][0] + BasicFeatures[sink][0]\n",
    "        num_of_in_neighbours_sum=BasicFeatures[source][4] + BasicFeatures[sink][4]\n",
    "        num_of_out_neighbours_sum=BasicFeatures[source][6] + BasicFeatures[sink][6]\n",
    "        \n",
    "        salton_similarity_score = salton_similarity(source, sink)\n",
    "        cosine = Cosine(source, sink)\n",
    "        jaccard_coefficient = get_jaccard_coefficient(source, sink)\n",
    "        preferential_attachment = get_preferential_attachment(source, sink)\n",
    "        adamic_adar = get_adamic_adar(source, sink)\n",
    "        resource_allocation = get_resource_allocation(source, sink)\n",
    "\n",
    "        X_test = pd.DataFrame([\n",
    "                               num_of_neighbours_source,\n",
    "                               num_of_in_neighbours_source,\n",
    "                               num_of_out_neighbours_source,\n",
    "                               num_of_neighbours_sink,\n",
    "                               num_of_in_neighbours_sink,\n",
    "                               num_of_out_neighbours_sink,\n",
    "                               num_of_neighbours_sum,\n",
    "                               num_of_in_neighbours_sum,\n",
    "                               num_of_out_neighbours_sum,      \n",
    "                               salton_similarity_score, \n",
    "                               cosine, \n",
    "                               jaccard_coefficient,\n",
    "                               preferential_attachment, \n",
    "                               adamic_adar, \n",
    "                               resource_allocation\n",
    "                              ]).T\n",
    "        single_result = grid_search.predict(X_test)[0]\n",
    "        result.append((line[0], single_result))\n",
    "    return result\n",
    "result = predict()\n",
    "\n",
    "\n",
    "# save the result\n",
    "\n",
    "import csv\n",
    "import time\n",
    "'''\n",
    "Description: get time\n",
    "Input: \n",
    "Output: time\n",
    "''' \n",
    "def nowtime():\n",
    "    return time.strftime(\"%Y%m%d-%H%M\", time.localtime())\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Description: Save prediction result to files\n",
    "Input: (1) result\n",
    "       (2) filename\n",
    "Output: \n",
    "\"\"\"\n",
    "def save_prediction_to_csv(result,filename):\n",
    "    headers = ['id','Prediction']\n",
    "\n",
    "    with open(filename + str(nowtime()) + \".csv\", 'w', encoding = 'utf8') as f:\n",
    "        f_csv = csv.writer(f)\n",
    "        f_csv.writerow(headers)\n",
    "        f_csv.writerows(result)\n",
    "save_prediction_to_csv(result, \"shawn_lr_all_standardise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the result of the prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Please save the training set as the csv file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
